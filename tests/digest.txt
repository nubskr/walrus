Directory structure:
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ integration.rs
    ‚îî‚îÄ‚îÄ unit.rs

================================================
FILE: integration.rs
================================================
use walrus::wal::Walrus;
use std::fs;
use std::thread;
use std::time::Duration;
use std::sync::Arc;

fn cleanup_wal() {
    let _ = fs::remove_dir_all("wal_files");
    // Give filesystem time to clean up
    thread::sleep(Duration::from_millis(10));
}

fn first_data_file() -> String {
    let mut files: Vec<_> = fs::read_dir("./wal_files").unwrap().flatten().collect();
    files.sort_by_key(|e| e.file_name());
    let p = files
        .into_iter()
        .find(|e| !e.file_name().to_string_lossy().ends_with("_index.db"))
        .unwrap()
        .path();
    p.to_string_lossy().to_string()
}

// ============================================================================
// INTEGRATION TESTS - END-TO-END SCENARIOS
// ============================================================================

#[test]
fn integration_basic_write_read_cycle() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Write some data
    wal.append_for_topic("test_topic", b"Hello, World!").unwrap();
    wal.append_for_topic("test_topic", b"Second message").unwrap();
    
    // Read it back
    let entry1 = wal.read_next("test_topic").unwrap();
    assert_eq!(entry1.data, b"Hello, World!");
    
    let entry2 = wal.read_next("test_topic").unwrap();
    assert_eq!(entry2.data, b"Second message");
    
    // Should be empty now
    assert!(wal.read_next("test_topic").is_none());
    
    cleanup_wal();
}

#[test]
fn integration_multiple_topics() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Write to different topics
    wal.append_for_topic("logs", b"Error occurred").unwrap();
    wal.append_for_topic("metrics", b"CPU: 80%").unwrap();
    wal.append_for_topic("logs", b"Warning issued").unwrap();
    wal.append_for_topic("events", b"User login").unwrap();
    
    // Read from each topic independently
    let log1 = wal.read_next("logs").unwrap();
    assert_eq!(log1.data, b"Error occurred");
    
    let metric1 = wal.read_next("metrics").unwrap();
    assert_eq!(metric1.data, b"CPU: 80%");
    
    let log2 = wal.read_next("logs").unwrap();
    assert_eq!(log2.data, b"Warning issued");
    
    let event1 = wal.read_next("events").unwrap();
    assert_eq!(event1.data, b"User login");
    
    // All topics should be empty now
    assert!(wal.read_next("logs").is_none());
    assert!(wal.read_next("metrics").is_none());
    assert!(wal.read_next("events").is_none());
    
    cleanup_wal();
}

#[test]
fn integration_empty_data_handling() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test empty data
    wal.append_for_topic("empty_test", b"").unwrap();
    let empty_entry = wal.read_next("empty_test").unwrap();
    assert!(empty_entry.data.is_empty());
    
    // Test single byte
    wal.append_for_topic("single_byte", &[42]).unwrap();
    let single_entry = wal.read_next("single_byte").unwrap();
    assert_eq!(single_entry.data, &[42]);
    
    cleanup_wal();
}

#[test]
fn integration_binary_data() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test binary data with various byte values
    let binary_data = vec![0, 1, 127, 128, 255, 0, 42];
    wal.append_for_topic("binary", &binary_data).unwrap();
    
    let entry = wal.read_next("binary").unwrap();
    assert_eq!(entry.data, binary_data);
    
    cleanup_wal();
}

#[test]
fn integration_utf8_strings() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test UTF-8 strings with special characters
    let utf8_strings = vec![
        "Hello, World!",
        "Caf√© ‚òï",
        "„Åì„Çì„Å´„Å°„ÅØ",
        "ü¶Ä Rust is awesome! üöÄ",
        "√ëo√±o ni√±o",
    ];
    
    for (i, s) in utf8_strings.iter().enumerate() {
        let topic = format!("utf8_{}", i);
        wal.append_for_topic(&topic, s.as_bytes()).unwrap();
    }
    
    for (i, expected) in utf8_strings.iter().enumerate() {
        let topic = format!("utf8_{}", i);
        let entry = wal.read_next(&topic).unwrap();
        let actual = String::from_utf8(entry.data).unwrap();
        assert_eq!(actual, *expected);
    }
    
    cleanup_wal();
}

#[test]
fn integration_medium_sized_data() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test with medium-sized data (1KB, 10KB, 100KB)
    let sizes = vec![1024, 10 * 1024, 100 * 1024];
    
    for (i, size) in sizes.iter().enumerate() {
        let data = vec![i as u8; *size];
        let topic = format!("medium_{}", i);
        wal.append_for_topic(&topic, &data).unwrap();
    }
    
    for (i, size) in sizes.iter().enumerate() {
        let expected = vec![i as u8; *size];
        let topic = format!("medium_{}", i);
        let entry = wal.read_next(&topic).unwrap();
        assert_eq!(entry.data, expected);
    }
    
    cleanup_wal();
}

#[test]
fn integration_sequential_writes_and_reads() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let topic = "sequential";
    
    // Write a sequence of messages
    for i in 0..20 {
        let message = format!("Message number {}", i);
        wal.append_for_topic(topic, message.as_bytes()).unwrap();
    }
    
    // Read them back in order
    for i in 0..20 {
        let expected = format!("Message number {}", i);
        let entry = wal.read_next(topic).unwrap();
        let actual = String::from_utf8(entry.data).unwrap();
        assert_eq!(actual, expected);
    }
    
    assert!(wal.read_next(topic).is_none());
    
    cleanup_wal();
}

#[test]
fn integration_interleaved_write_read() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let topic = "interleaved";
    
    // Write some messages
    wal.append_for_topic(topic, b"Message 1").unwrap();
    wal.append_for_topic(topic, b"Message 2").unwrap();
    
    // Read one
    let entry1 = wal.read_next(topic).unwrap();
    assert_eq!(entry1.data, b"Message 1");
    
    // Write more
    wal.append_for_topic(topic, b"Message 3").unwrap();
    wal.append_for_topic(topic, b"Message 4").unwrap();
    
    // Read the rest
    let entry2 = wal.read_next(topic).unwrap();
    assert_eq!(entry2.data, b"Message 2");
    
    let entry3 = wal.read_next(topic).unwrap();
    assert_eq!(entry3.data, b"Message 3");
    
    let entry4 = wal.read_next(topic).unwrap();
    assert_eq!(entry4.data, b"Message 4");
    
    assert!(wal.read_next(topic).is_none());
    
    cleanup_wal();
}

#[test]
fn integration_multiple_topics_stress() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let num_topics = 5;
    let messages_per_topic = 10;
    
    // Write to multiple topics
    for topic_id in 0..num_topics {
        for msg_id in 0..messages_per_topic {
            let topic = format!("stress_topic_{}", topic_id);
            let message = format!("Topic {} Message {}", topic_id, msg_id);
            wal.append_for_topic(&topic, message.as_bytes()).unwrap();
        }
    }
    
    // Read from all topics and verify
    for topic_id in 0..num_topics {
        let topic = format!("stress_topic_{}", topic_id);
        for msg_id in 0..messages_per_topic {
            let expected = format!("Topic {} Message {}", topic_id, msg_id);
            let entry = wal.read_next(&topic).unwrap();
            let actual = String::from_utf8(entry.data).unwrap();
            assert_eq!(actual, expected);
        }
        assert!(wal.read_next(&topic).is_none());
    }
    
    cleanup_wal();
}

#[test]
fn integration_concurrent_writes() {
    cleanup_wal();
    
    let wal = Arc::new(Walrus::new());
    let num_threads = 3;
    let messages_per_thread = 5;
    
    let mut handles = vec![];
    
    // Spawn threads that write to different topics
    for thread_id in 0..num_threads {
        let wal_clone = Arc::clone(&wal);
        let handle = thread::spawn(move || {
            let topic = format!("concurrent_{}", thread_id);
            for msg_id in 0..messages_per_thread {
                let message = format!("Thread {} Message {}", thread_id, msg_id);
                wal_clone.append_for_topic(&topic, message.as_bytes()).unwrap();
                // Small delay to allow interleaving
                thread::sleep(Duration::from_millis(1));
            }
        });
        handles.push(handle);
    }
    
    // Wait for all threads to complete
    for handle in handles {
        handle.join().unwrap();
    }
    
    // Verify all messages were written correctly
    for thread_id in 0..num_threads {
        let topic = format!("concurrent_{}", thread_id);
        for msg_id in 0..messages_per_thread {
            let expected = format!("Thread {} Message {}", thread_id, msg_id);
            let entry = wal.read_next(&topic).unwrap();
            let actual = String::from_utf8(entry.data).unwrap();
            assert_eq!(actual, expected);
        }
        assert!(wal.read_next(&topic).is_none());
    }
    
    cleanup_wal();
}

#[test]
fn integration_topic_isolation() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Write to multiple topics
    wal.append_for_topic("topic_a", b"A1").unwrap();
    wal.append_for_topic("topic_b", b"B1").unwrap();
    wal.append_for_topic("topic_a", b"A2").unwrap();
    wal.append_for_topic("topic_c", b"C1").unwrap();
    wal.append_for_topic("topic_b", b"B2").unwrap();
    
    // Read from topic_a only
    assert_eq!(wal.read_next("topic_a").unwrap().data, b"A1");
    assert_eq!(wal.read_next("topic_a").unwrap().data, b"A2");
    assert!(wal.read_next("topic_a").is_none());
    
    // Other topics should still have their data
    assert_eq!(wal.read_next("topic_b").unwrap().data, b"B1");
    assert_eq!(wal.read_next("topic_b").unwrap().data, b"B2");
    assert!(wal.read_next("topic_b").is_none());
    
    assert_eq!(wal.read_next("topic_c").unwrap().data, b"C1");
    assert!(wal.read_next("topic_c").is_none());
    
    cleanup_wal();
}

#[test]
fn integration_nonexistent_topic() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Reading from a topic that doesn't exist should return None
    assert!(wal.read_next("nonexistent").is_none());
    
    // Write to a topic, then read from a different one
    wal.append_for_topic("existing", b"data").unwrap();
    assert!(wal.read_next("different").is_none());
    
    // The original topic should still have data
    assert_eq!(wal.read_next("existing").unwrap().data, b"data");
    
    cleanup_wal();
}

#[test]
fn integration_write_after_exhaustion() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let topic = "exhaustion_test";
    
    // Write and read all data
    wal.append_for_topic(topic, b"first").unwrap();
    assert_eq!(wal.read_next(topic).unwrap().data, b"first");
    assert!(wal.read_next(topic).is_none());
    
    // Write more data after exhaustion
    wal.append_for_topic(topic, b"second").unwrap();
    wal.append_for_topic(topic, b"third").unwrap();
    
    // Should be able to read new data
    assert_eq!(wal.read_next(topic).unwrap().data, b"second");
    assert_eq!(wal.read_next(topic).unwrap().data, b"third");
    assert!(wal.read_next(topic).is_none());
    
    cleanup_wal();
}

#[test]
fn integration_large_topic_names() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test with reasonably long topic names (but not too long to exceed metadata limits)
    let long_topic = "a".repeat(15);  // Reduced to stay within metadata limits
    let very_long_topic = "b".repeat(18);  // Reduced to stay within metadata limits
    
    wal.append_for_topic(&long_topic, b"long topic data").unwrap();
    wal.append_for_topic(&very_long_topic, b"very long topic data").unwrap();
    
    assert_eq!(wal.read_next(&long_topic).unwrap().data, b"long topic data");
    assert_eq!(wal.read_next(&very_long_topic).unwrap().data, b"very long topic data");
    
    cleanup_wal();
}

// ============================================================================
// EXTREME INTEGRATION STRESS TESTS - ABSOLUTE LIMITS
// ============================================================================

#[test]
fn integration_memory_pressure_test() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let num_topics = 100;
    let large_entry_size = 1024 * 1024; // 1MB per entry
    
    // Create memory pressure with large entries across many topics
    for topic_id in 0..num_topics {
        let topic = format!("memory_pressure_{}", topic_id);
        
        // Create large entry with validation pattern
        let mut data = Vec::with_capacity(large_entry_size);
        for i in 0..large_entry_size {
            data.push(((topic_id + i) % 256) as u8);
        }
        
        wal.append_for_topic(&topic, &data).unwrap();
    }
    
    // Read back and validate all large entries
    for topic_id in 0..num_topics {
        let topic = format!("memory_pressure_{}", topic_id);
        let entry = wal.read_next(&topic).unwrap();
        
        assert_eq!(entry.data.len(), large_entry_size);
        
        // Validate pattern
        for (i, &byte) in entry.data.iter().enumerate() {
            assert_eq!(byte, ((topic_id + i) % 256) as u8,
                      "Memory pressure test failed at topic {} byte {}", topic_id, i);
        }
    }
    
    cleanup_wal();
}

#[test]
fn integration_file_rollover_stress() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let topic = "rollover_stress";
    
    // Force multiple file rollovers with large entries
    let entry_size = 50 * 1024 * 1024; // 50MB entries to force rollovers
    let num_entries = 5;
    
    for entry_id in 0..num_entries {
        let mut data = Vec::with_capacity(entry_size);
        
        // Fill with entry-specific pattern
        for i in 0..entry_size {
            data.push(((entry_id * 1000 + i) % 256) as u8);
        }
        
        wal.append_for_topic(topic, &data).unwrap();
    }
    
    // Read back across file boundaries
    for entry_id in 0..num_entries {
        let entry = wal.read_next(topic).unwrap();
        assert_eq!(entry.data.len(), entry_size);
        
        // Validate pattern across file boundaries
        for (i, &byte) in entry.data.iter().enumerate() {
            assert_eq!(byte, ((entry_id * 1000 + i) % 256) as u8,
                      "File rollover validation failed at entry {} byte {}", entry_id, i);
        }
    }
    
    cleanup_wal();
}

#[test]
fn integration_corruption_detection_comprehensive() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let topic = "corruption_test";
    
    // Write data with strong validation patterns
    let test_data = b"CORRUPTION_TEST_DATA_WITH_STRONG_PATTERN_12345678901234567890";
    wal.append_for_topic(topic, test_data).unwrap();
    
    // Verify normal read works
    let entry = wal.read_next(topic).unwrap();
    assert_eq!(entry.data, test_data);
    
    // Now test corruption detection by corrupting the file
    let path = first_data_file();
    let mut file_data = std::fs::read(&path).unwrap();
    
    // Find the test data in the file
    if let Some(pos) = file_data.windows(test_data.len()).position(|w| w == test_data) {
        // Corrupt multiple bytes to ensure detection
        for i in 0..5 {
            if pos + i < file_data.len() {
                file_data[pos + i] ^= 0xFF; // Flip all bits
            }
        }
        
        std::fs::write(&path, &file_data).unwrap();
        
        // Create new WAL instance and try to read
        let wal2 = Walrus::new();
        
        // Should detect corruption and handle gracefully
        match wal2.read_next(topic) {
            None => {
                // Expected: corruption detected, no data returned
            }
            Some(corrupted_entry) => {
                // If data is returned, it should be different from original
                assert_ne!(corrupted_entry.data, test_data, 
                          "Corruption not detected - data should be different");
            }
        }
    }
    
    cleanup_wal();
}

#[test]
fn integration_extreme_topic_count() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let num_topics = 5000; // Extreme number of topics
    
    // Write one entry per topic with validation data
    for topic_id in 0..num_topics {
        let topic = format!("extreme_topic_{:06}", topic_id);
        
        let mut data = Vec::new();
        data.extend_from_slice(&(topic_id as u64).to_le_bytes());
        data.extend_from_slice(format!("TOPIC_DATA_{}", topic_id).as_bytes());
        
        wal.append_for_topic(&topic, &data).unwrap();
    }
    
    // Read back in random order to stress topic lookup
    let mut read_order: Vec<usize> = (0..num_topics).collect();
    
    // Simple shuffle using topic_id as seed
    for i in 0..num_topics {
        let j = (i * 1103515245 + 12345) % num_topics;
        read_order.swap(i, j);
    }
    
    for &topic_id in &read_order {
        let topic = format!("extreme_topic_{:06}", topic_id);
        let entry = wal.read_next(&topic).unwrap();
        
        // Validate embedded topic ID
        let read_topic_id = u64::from_le_bytes([
            entry.data[0], entry.data[1], entry.data[2], entry.data[3],
            entry.data[4], entry.data[5], entry.data[6], entry.data[7]
        ]);
        
        assert_eq!(read_topic_id, topic_id as u64);
        
        // Validate payload
        let expected_payload = format!("TOPIC_DATA_{}", topic_id);
        let actual_payload = String::from_utf8(entry.data[8..].to_vec()).unwrap();
        assert_eq!(actual_payload, expected_payload);
    }
    
    cleanup_wal();
}

#[test]
fn integration_mixed_size_stress() {
    cleanup_wal();
    
    let wal = Walrus::new();
    let topic = "mixed_sizes";
    
    // Test with exponentially increasing sizes
    let base_sizes = vec![1, 10, 100, 1000, 10000, 100000, 1000000];
    
    for (i, &base_size) in base_sizes.iter().enumerate() {
        let mut data = Vec::with_capacity(base_size);
        
        // Fill with size-dependent pattern
        for j in 0..base_size {
            data.push(((i * 1000 + j) % 256) as u8);
        }
        
        wal.append_for_topic(topic, &data).unwrap();
    }
    
    // Read back and validate each size
    for (i, &base_size) in base_sizes.iter().enumerate() {
        let entry = wal.read_next(topic).unwrap();
        assert_eq!(entry.data.len(), base_size);
        
        for (j, &byte) in entry.data.iter().enumerate() {
            assert_eq!(byte, ((i * 1000 + j) % 256) as u8,
                      "Mixed size validation failed at size {} byte {}", base_size, j);
        }
    }
    
    cleanup_wal();
}

#[test]
fn integration_persistence_stress_with_validation() {
    cleanup_wal();
    
    // Phase 1: Write lots of data
    {
        let wal = Walrus::new();
        let num_topics = 100;
        let entries_per_topic = 50;
        
        for topic_id in 0..num_topics {
            let topic = format!("persist_stress_{}", topic_id);
            
            for entry_id in 0..entries_per_topic {
                let mut data = Vec::new();
                data.extend_from_slice(&(topic_id as u32).to_le_bytes());
                data.extend_from_slice(&(entry_id as u32).to_le_bytes());
                
                // Add timestamp-like data
                let timestamp = (topic_id * 1000 + entry_id) as u64;
                data.extend_from_slice(&timestamp.to_le_bytes());
                
                // Add payload
                let payload = format!("PERSIST_{}_{}", topic_id, entry_id);
                data.extend_from_slice(payload.as_bytes());
                
                wal.append_for_topic(&topic, &data).unwrap();
            }
        }
        
        // Read some data to advance read positions
        for topic_id in 0..num_topics {
            let topic = format!("persist_stress_{}", topic_id);
            
            // Read half the entries
            for _ in 0..(entries_per_topic / 2) {
                wal.read_next(&topic).unwrap();
            }
        }
    } // WAL instance dropped here
    
    // Phase 2: Restart and validate persistence
    {
        let wal = Walrus::new();
        let num_topics = 100;
        let entries_per_topic = 50;
        
        // Continue reading from where we left off
        for topic_id in 0..num_topics {
            let topic = format!("persist_stress_{}", topic_id);
            
            // Read remaining entries
            for entry_id in (entries_per_topic / 2)..entries_per_topic {
                let entry = wal.read_next(&topic).unwrap();
                
                // Validate all embedded data
                let read_topic_id = u32::from_le_bytes([
                    entry.data[0], entry.data[1], entry.data[2], entry.data[3]
                ]);
                let read_entry_id = u32::from_le_bytes([
                    entry.data[4], entry.data[5], entry.data[6], entry.data[7]
                ]);
                let read_timestamp = u64::from_le_bytes([
                    entry.data[8], entry.data[9], entry.data[10], entry.data[11],
                    entry.data[12], entry.data[13], entry.data[14], entry.data[15]
                ]);
                
                assert_eq!(read_topic_id, topic_id as u32);
                assert_eq!(read_entry_id, entry_id as u32);
                assert_eq!(read_timestamp, (topic_id * 1000 + entry_id) as u64);
                
                // Validate payload
                let expected_payload = format!("PERSIST_{}_{}", topic_id, entry_id);
                let actual_payload = String::from_utf8(entry.data[16..].to_vec()).unwrap();
                assert_eq!(actual_payload, expected_payload);
            }
            
            // Verify no more entries
            assert!(wal.read_next(&topic).is_none());
        }
    }
    
    cleanup_wal();
}

#[test]
fn integration_data_pattern_stress() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test various challenging data patterns
    let patterns = vec![
        ("all_zeros", vec![0u8; 10000]),
        ("all_ones", vec![0xFF; 10000]),
        ("alternating_bytes", (0..10000).map(|i| if i % 2 == 0 { 0x00 } else { 0xFF }).collect()),
        ("incremental", (0..10000).map(|i| (i % 256) as u8).collect()),
        ("decremental", (0..10000).map(|i| (255 - (i % 256)) as u8).collect()),
        ("repeating_pattern", vec![0xAA, 0xBB, 0xCC, 0xDD].repeat(2500)),
        ("pseudo_random", {
            let mut data = Vec::new();
            let mut seed = 0x12345678u32;
            for _ in 0..10000 {
                seed = seed.wrapping_mul(1664525).wrapping_add(1013904223);
                data.push((seed >> 24) as u8);
            }
            data
        }),
    ];
    
    // Write all patterns
    for (pattern_name, data) in &patterns {
        wal.append_for_topic(pattern_name, data).unwrap();
    }
    
    // Read back and validate
    for (pattern_name, expected_data) in patterns {
        let entry = wal.read_next(&pattern_name).unwrap();
        assert_eq!(entry.data, expected_data, 
                  "Pattern '{}' was corrupted during storage/retrieval", pattern_name);
    }
    
    cleanup_wal();
}

#[test]
fn integration_special_topic_names() {
    cleanup_wal();
    
    let wal = Walrus::new();
    
    // Test with special characters in topic names
    let topics = vec![
        "topic-with-dashes",
        "topic_with_underscores",
        "topic.with.dots",
        "topic123",
        "UPPERCASE_TOPIC",
        "MixedCaseTopic",
    ];
    
    for (i, topic) in topics.iter().enumerate() {
        let data = format!("Data for topic {}", i);
        wal.append_for_topic(topic, data.as_bytes()).unwrap();
    }
    
    for (i, topic) in topics.iter().enumerate() {
        let expected = format!("Data for topic {}", i);
        let entry = wal.read_next(topic).unwrap();
        let actual = String::from_utf8(entry.data).unwrap();
        assert_eq!(actual, expected);
    }
    
    cleanup_wal();
}


================================================
FILE: unit.rs
================================================
use walrus::wal::{Walrus, WalIndex, Entry};
use std::fs::{self, OpenOptions};
use std::io::{Seek, SeekFrom, Write, Read};

fn cleanup_wal() {
    let _ = fs::remove_dir_all("wal_files");
}

fn first_data_file() -> String {
    let mut files: Vec<_> = fs::read_dir("./wal_files").unwrap().flatten().collect();
    files.sort_by_key(|e| e.file_name());
    let p = files
        .into_iter()
        .find(|e| !e.file_name().to_string_lossy().ends_with("_index.db"))
        .unwrap()
        .path();
    p.to_string_lossy().to_string()
}

#[test]
fn walindex_persists() {
    fs::create_dir_all("wal_files").unwrap();
    let name = format!("unit_idx_{}", {
        use std::time::SystemTime;
        SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).unwrap().as_millis()
    });
    let mut idx = WalIndex::new(&name);
    idx.set("k".to_string(), 7, 99);
    drop(idx);
    let idx2 = WalIndex::new(&name);
    let bp = idx2.get("k").unwrap();
    assert_eq!(bp.cur_block_idx, 7);
    assert_eq!(bp.cur_block_offset, 99);
    cleanup_wal();
}

#[test]
fn large_entry_forces_block_seal() {
    cleanup_wal();
    let wal = Walrus::new();
    
    // Create 9MB entries to force block sealing
    let large_data_1 = vec![0x42u8; 9 * 1024 * 1024]; // 9MB of 0x42
    let large_data_2 = vec![0x43u8; 9 * 1024 * 1024]; // 9MB of 0x43
    let large_data_3 = vec![0x43u8; 9 * 1024 * 1024]; // 9MB of 0x43

    // add a 2 second timeout
    
    wal.append_for_topic("t", &large_data_1).unwrap();
    wal.append_for_topic("t", &large_data_2).unwrap();
    wal.append_for_topic("t", &large_data_3).unwrap();
    
    // std::thread::sleep(std::time::Duration::from_secs(1));

    assert_eq!(wal.read_next("t").unwrap().data, large_data_1);
    assert_eq!(wal.read_next("t").unwrap().data, large_data_2);
    assert_eq!(wal.read_next("t").unwrap().data, large_data_3); // it will fail because it's in the write block still :))
    
    cleanup_wal();
}

#[test]
fn basic_roundtrip_single_topic() {
    cleanup_wal();
    let wal = Walrus::new();
    wal.append_for_topic("t", b"x").unwrap();
    wal.append_for_topic("t", b"y").unwrap();
    assert_eq!(wal.read_next("t").unwrap().data, b"x");
    assert_eq!(wal.read_next("t").unwrap().data, b"y");
    assert!(wal.read_next("t").is_none());
    cleanup_wal();
}

#[test]
fn basic_roundtrip_multi_topic() {
    cleanup_wal();
    let wal = Walrus::new();
    wal.append_for_topic("a", b"1").unwrap();
    wal.append_for_topic("b", b"2").unwrap();
    assert_eq!(wal.read_next("a").unwrap().data, b"1");
    assert_eq!(wal.read_next("b").unwrap().data, b"2");
    cleanup_wal();
}

#[test]
fn persists_read_offsets_across_restart() {
    cleanup_wal();
    let wal = Walrus::new();
    wal.append_for_topic("t", b"a").unwrap();
    wal.append_for_topic("t", b"b").unwrap();
    assert_eq!(wal.read_next("t").unwrap().data, b"a");
    // restart
    let wal2 = Walrus::new();
    assert_eq!(wal2.read_next("t").unwrap().data, b"b");
    assert!(wal2.read_next("t").is_none());
    cleanup_wal();
}

#[test]
fn checksum_corruption_is_detected_via_public_api() {
    cleanup_wal();
    let wal = Walrus::new();
    wal.append_for_topic("t", b"abcdef").unwrap();
    // corrupt by finding the data pattern in the file and flipping a byte
    let path = first_data_file();
    let mut bytes = Vec::new();
    {
        let mut f = OpenOptions::new().read(true).open(&path).unwrap();
        f.read_to_end(&mut bytes).unwrap();
    }
    if let Some(pos) = bytes.windows(6).position(|w| w == b"abcdef") {
        // flip one byte in the middle of the payload
        let flip_pos = pos + 2;
        let mut f = OpenOptions::new().read(true).write(true).open(&path).unwrap();
        f.seek(SeekFrom::Start(flip_pos as u64)).unwrap();
        f.write_all(&[bytes[flip_pos] ^ 0xFF]).unwrap();
    } else {
        panic!("payload not found to corrupt");
    }
    // restart and try reading
    let wal2 = Walrus::new();
    let res = wal2.read_next("t");
    assert!(res.is_none());
    cleanup_wal();
}

// ============================================================================
// EXTREME STRESS TESTS - PUSHING WAL TO ABSOLUTE LIMITS
// ============================================================================

#[test]
fn stress_massive_single_entry() {
    cleanup_wal();
    let wal = Walrus::new();
    
    // Create a massive entry (close to 1GB limit)
    let size = 100 * 1024 * 1024; // 100MB
    let mut massive_data = Vec::with_capacity(size);
    
    // Fill with pattern that's easy to validate
    for i in 0..size {
        massive_data.push((i % 256) as u8);
    }
    
    // Write the massive entry
    wal.append_for_topic("massive", &massive_data).unwrap();
    
    // Read it back and validate every byte
    let entry = wal.read_next("massive").unwrap();
    assert_eq!(entry.data.len(), size);
    
    for (i, &byte) in entry.data.iter().enumerate() {
        assert_eq!(byte, (i % 256) as u8, "Data corruption at byte {}", i);
    }
    
    cleanup_wal();
}

#[test]
fn stress_many_topics_with_validation() {
    cleanup_wal();
    let wal = Walrus::new();
    
    let num_topics = 1000;
    let entries_per_topic = 100;
    
    // Write data with predictable patterns
    for topic_id in 0..num_topics {
        let topic = format!("topic_{:04}", topic_id);
        
        for entry_id in 0..entries_per_topic {
            // Create data with topic_id and entry_id embedded
            let mut data = Vec::new();
            data.extend_from_slice(&(topic_id as u32).to_le_bytes());
            data.extend_from_slice(&(entry_id as u32).to_le_bytes());
            
            // Add some payload with checksum
            let payload = format!("data_{}_{}_", topic_id, entry_id).repeat(10);
            data.extend_from_slice(payload.as_bytes());
            
            wal.append_for_topic(&topic, &data).unwrap();
        }
    }
    
    // Read back and validate all data
    for topic_id in 0..num_topics {
        let topic = format!("topic_{:04}", topic_id);
        
        for entry_id in 0..entries_per_topic {
            let entry = wal.read_next(&topic).unwrap();
            
            // Validate embedded IDs
            let read_topic_id = u32::from_le_bytes([
                entry.data[0], entry.data[1], entry.data[2], entry.data[3]
            ]);
            let read_entry_id = u32::from_le_bytes([
                entry.data[4], entry.data[5], entry.data[6], entry.data[7]
            ]);
            
            assert_eq!(read_topic_id, topic_id as u32);
            assert_eq!(read_entry_id, entry_id as u32);
            
            // Validate payload
            let expected_payload = format!("data_{}_{}_", topic_id, entry_id).repeat(10);
            let actual_payload = String::from_utf8(entry.data[8..].to_vec()).unwrap();
            assert_eq!(actual_payload, expected_payload);
        }
        
        // Verify no more entries
        assert!(wal.read_next(&topic).is_none());
    }
    
    cleanup_wal();
}

#[test]
fn stress_rapid_write_read_cycles() {
    cleanup_wal();
    let wal = Walrus::new();
    
    let cycles = 10000;
    let topic = "rapid_cycles";
    
    for cycle in 0..cycles {
        // Write with cycle number and timestamp-like data
        let mut data = Vec::new();
        data.extend_from_slice(&(cycle as u64).to_le_bytes());
        data.extend_from_slice(&[0xAA, 0xBB, 0xCC, 0xDD]); // Magic bytes
        
        // Add variable-length payload
        let payload_size = (cycle % 100) + 1;
        for i in 0..payload_size {
            data.push((cycle + i) as u8);
        }
        
        wal.append_for_topic(topic, &data).unwrap();
        
        // Immediately read back and validate
        let entry = wal.read_next(topic).unwrap();
        
        // Validate cycle number
        let read_cycle = u64::from_le_bytes([
            entry.data[0], entry.data[1], entry.data[2], entry.data[3],
            entry.data[4], entry.data[5], entry.data[6], entry.data[7]
        ]);
        assert_eq!(read_cycle, cycle as u64);
        
        // Validate magic bytes
        assert_eq!(&entry.data[8..12], &[0xAA, 0xBB, 0xCC, 0xDD]);
        
        // Validate payload
        let expected_payload_size = (cycle % 100) + 1;
        assert_eq!(entry.data.len(), 8 + 4 + expected_payload_size);
        
        for (i, &byte) in entry.data[12..].iter().enumerate() {
            assert_eq!(byte, ((cycle + i) % 256) as u8);
        }
    }
    
    cleanup_wal();
}

#[test]
fn stress_boundary_conditions() {
    cleanup_wal();
    let wal = Walrus::new();
    
    // Test various boundary sizes
    let test_sizes = vec![
        0,                    // Empty
        1,                    // Single byte
        63,                   // Just under metadata size
        64,                   // Exactly metadata size
        65,                   // Just over metadata size
        1023,                 // Just under 1KB
        1024,                 // Exactly 1KB
        1025,                 // Just over 1KB
        65535,                // Just under 64KB
        65536,                // Exactly 64KB
        65537,                // Just over 64KB
        1024 * 1024 - 1,      // Just under 1MB
        1024 * 1024,          // Exactly 1MB
        1024 * 1024 + 1,      // Just over 1MB
    ];
    
    for (i, &size) in test_sizes.iter().enumerate() {
        let topic = format!("boundary_{}", i);
        
        // Create data with size-specific pattern
        let mut data = Vec::with_capacity(size);
        for j in 0..size {
            data.push(((i + j) % 256) as u8);
        }
        
        wal.append_for_topic(&topic, &data).unwrap();
        
        // Read back and validate
        let entry = wal.read_next(&topic).unwrap();
        assert_eq!(entry.data.len(), size);
        
        for (j, &byte) in entry.data.iter().enumerate() {
            assert_eq!(byte, ((i + j) % 256) as u8, 
                      "Mismatch at size {} byte {}", size, j);
        }
    }
    
    cleanup_wal();
}

#[test]
fn stress_data_integrity_patterns() {
    cleanup_wal();
    let wal = Walrus::new();
    
    // Test various data patterns that might expose corruption
    let patterns = vec![
        ("zeros", vec![0u8; 1000]),
        ("ones", vec![0xFF; 1000]),
        ("alternating", (0..1000).map(|i| if i % 2 == 0 { 0xAA } else { 0x55 }).collect()),
        ("sequential", (0..1000).map(|i| (i % 256) as u8).collect()),
        ("reverse", (0..1000).map(|i| (255 - (i % 256)) as u8).collect()),
        ("random_seed", {
            let mut data = Vec::new();
            let mut seed = 12345u32;
            for _ in 0..1000 {
                seed = seed.wrapping_mul(1103515245).wrapping_add(12345);
                data.push((seed >> 16) as u8);
            }
            data
        }),
    ];
    
    for (pattern_name, data) in patterns {
        wal.append_for_topic(pattern_name, &data).unwrap();
        
        let entry = wal.read_next(pattern_name).unwrap();
        assert_eq!(entry.data, data, "Pattern {} corrupted", pattern_name);
    }
    
    cleanup_wal();
}

#[test]
fn stress_concurrent_topic_validation() {
    cleanup_wal();
    let wal = Walrus::new();
    
    let num_topics = 50;
    let entries_per_topic = 200;
    
    // Write interleaved data
    for round in 0..entries_per_topic {
        for topic_id in 0..num_topics {
            let topic = format!("concurrent_{}", topic_id);
            
            // Create data with round and topic embedded
            let mut data = Vec::new();
            data.extend_from_slice(&(topic_id as u32).to_le_bytes());
            data.extend_from_slice(&(round as u32).to_le_bytes());
            
            // Add checksum-like data
            let checksum = (topic_id + round) % 256;
            data.push(checksum as u8);
            
            // Add payload
            let payload = format!("T{}R{}", topic_id, round);
            data.extend_from_slice(payload.as_bytes());
            
            wal.append_for_topic(&topic, &data).unwrap();
        }
    }
    
    // Read back in topic order and validate
    for topic_id in 0..num_topics {
        let topic = format!("concurrent_{}", topic_id);
        
        for round in 0..entries_per_topic {
            let entry = wal.read_next(&topic).unwrap();
            
            // Validate embedded data
            let read_topic_id = u32::from_le_bytes([
                entry.data[0], entry.data[1], entry.data[2], entry.data[3]
            ]);
            let read_round = u32::from_le_bytes([
                entry.data[4], entry.data[5], entry.data[6], entry.data[7]
            ]);
            let read_checksum = entry.data[8];
            
            assert_eq!(read_topic_id, topic_id as u32);
            assert_eq!(read_round, round as u32);
            assert_eq!(read_checksum, ((topic_id + round) % 256) as u8);
            
            // Validate payload
            let expected_payload = format!("T{}R{}", topic_id, round);
            let actual_payload = String::from_utf8(entry.data[9..].to_vec()).unwrap();
            assert_eq!(actual_payload, expected_payload);
        }
    }
    
    cleanup_wal();
}

#[test]
fn stress_extreme_topic_names() {
    cleanup_wal();
    let wal = Walrus::new();
    
    // Test various extreme topic names
    let extreme_topics = vec![
        "a".to_string(),                                    // Single char
        "a".repeat(10),                                     // Short
        "topic_with_underscores_and_numbers_123".to_string(), // Mixed
        "UPPERCASE_TOPIC".to_string(),                      // Uppercase
        "mixed_Case_Topic_123".to_string(),                // Mixed case
        "topic.with.dots".to_string(),                     // Dots
        "topic-with-dashes".to_string(),                   // Dashes
        "0123456789".to_string(),                          // Numbers only
        "topic_with_unicode_caf√©".to_string(),             // Unicode (if supported)
    ];
    
    for (i, topic) in extreme_topics.iter().enumerate() {
        let data = format!("data_for_topic_{}", i).as_bytes().to_vec();
        
        match wal.append_for_topic(topic, &data) {
            Ok(_) => {
                let entry = wal.read_next(topic).unwrap();
                assert_eq!(entry.data, data);
            }
            Err(_) => {
                // Some topic names might be invalid, that's okay
                println!("Topic '{}' rejected (expected for some cases)", topic);
            }
        }
    }
    
    cleanup_wal();
}

// ============================================================================
// UNIT TESTS FOR INDIVIDUAL COMPONENTS
// ============================================================================

mod checksum_tests {
    use super::*;
    
    // We need to access the internal checksum function, so we'll test it indirectly
    // through the public API by verifying data integrity
    #[test]
    fn checksum_detects_corruption() {
        cleanup_wal();
        let wal = Walrus::new();
        
        // Write some data
        let test_data = b"test_checksum_data_12345";
        wal.append_for_topic("checksum_test", test_data).unwrap();
        
        // First verify we can read the data normally
        let entry = wal.read_next("checksum_test").unwrap();
        assert_eq!(entry.data, test_data);
        
        // Corrupt the file and verify checksum detection
        let path = first_data_file();
        let mut bytes = Vec::new();
        {
            let mut f = OpenOptions::new().read(true).open(&path).unwrap();
            f.read_to_end(&mut bytes).unwrap();
        }
        
        // Find and corrupt the data - corrupt multiple bytes to ensure detection
        if let Some(pos) = bytes.windows(test_data.len()).position(|w| w == test_data) {
            let mut f = OpenOptions::new().read(true).write(true).open(&path).unwrap();
            f.seek(SeekFrom::Start(pos as u64)).unwrap();
            // Corrupt the first few bytes of the data
            let corrupted = [test_data[0] ^ 0xFF, test_data[1] ^ 0xFF, test_data[2] ^ 0xFF];
            f.write_all(&corrupted).unwrap();
            f.sync_all().unwrap(); // Ensure the corruption is written to disk
        } else {
            panic!("Test data not found in file for corruption");
        }
        
        // Restart and verify corruption is detected
        let wal2 = Walrus::new();
        let result = wal2.read_next("checksum_test");
        
        // The corrupted data should either return None or the corrupted data should be different
        match result {
            None => {
                // Corruption detected - this is what we expect
            }
            Some(entry) => {
                // If we get data back, it should be different from the original
                assert_ne!(entry.data, test_data, "Corruption was not detected - got original data back");
            }
        }
        
        cleanup_wal();
    }
}

mod entry_tests {
    use super::*;
    
    #[test]
    fn entry_creation_and_data_access() {
        let test_data = vec![1, 2, 3, 4, 5];
        let entry = Entry { data: test_data.clone() };
        
        assert_eq!(entry.data, test_data);
        assert_eq!(entry.data.len(), 5);
    }
    
    #[test]
    fn entry_with_empty_data() {
        let entry = Entry { data: Vec::new() };
        assert!(entry.data.is_empty());
    }
    
    #[test]
    fn entry_with_large_data() {
        let large_data = vec![42u8; 1024 * 1024]; // 1MB
        let entry = Entry { data: large_data.clone() };
        assert_eq!(entry.data.len(), 1024 * 1024);
        assert_eq!(entry.data[0], 42);
        assert_eq!(entry.data[1024 * 1024 - 1], 42);
    }
}

mod wal_index_tests {
    use super::*;
    
    #[test]
    fn wal_index_basic_operations() {
        cleanup_wal();
        let mut idx = WalIndex::new("test_basic");
        
        // Test set and get
        idx.set("key1".to_string(), 10, 20);
        let pos = idx.get("key1").unwrap();
        assert_eq!(pos.cur_block_idx, 10);
        assert_eq!(pos.cur_block_offset, 20);
        
        // Test non-existent key
        assert!(idx.get("nonexistent").is_none());
        
        cleanup_wal();
    }
    
    #[test]
    fn wal_index_update_existing_key() {
        cleanup_wal();
        let mut idx = WalIndex::new("test_update");
        
        idx.set("key1".to_string(), 10, 20);
        idx.set("key1".to_string(), 30, 40); // Update
        
        let pos = idx.get("key1").unwrap();
        assert_eq!(pos.cur_block_idx, 30);
        assert_eq!(pos.cur_block_offset, 40);
        
        cleanup_wal();
    }
    
    #[test]
    fn wal_index_remove_key() {
        cleanup_wal();
        let mut idx = WalIndex::new("test_remove");
        
        idx.set("key1".to_string(), 10, 20);
        let removed = idx.remove("key1").unwrap();
        assert_eq!(removed.cur_block_idx, 10);
        assert_eq!(removed.cur_block_offset, 20);
        
        assert!(idx.get("key1").is_none());
        assert!(idx.remove("key1").is_none()); // Remove non-existent
        
        cleanup_wal();
    }
    
    #[test]
    fn wal_index_persistence_across_instances() {
        cleanup_wal();
        let index_name = "test_persistence";
        
        // Create and populate index
        {
            let mut idx = WalIndex::new(index_name);
            idx.set("persistent_key".to_string(), 100, 200);
        }
        
        // Create new instance and verify data persists
        {
            let idx = WalIndex::new(index_name);
            let pos = idx.get("persistent_key").unwrap();
            assert_eq!(pos.cur_block_idx, 100);
            assert_eq!(pos.cur_block_offset, 200);
        }
        
        cleanup_wal();
    }
    
    #[test]
    fn wal_index_multiple_keys() {
        cleanup_wal();
        let mut idx = WalIndex::new("test_multiple");
        
        idx.set("key1".to_string(), 10, 20);
        idx.set("key2".to_string(), 30, 40);
        idx.set("key3".to_string(), 50, 60);
        
        assert_eq!(idx.get("key1").unwrap().cur_block_idx, 10);
        assert_eq!(idx.get("key2").unwrap().cur_block_idx, 30);
        assert_eq!(idx.get("key3").unwrap().cur_block_idx, 50);
        
        cleanup_wal();
    }
}

mod walrus_integration_tests {
    use super::*;
    
    #[test]
    fn walrus_empty_topic_read() {
        cleanup_wal();
        let wal = Walrus::new();
        
        // Reading from non-existent topic should return None
        assert!(wal.read_next("empty_topic").is_none());
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_single_entry_per_topic() {
        cleanup_wal();
        let wal = Walrus::new();
        
        wal.append_for_topic("topic1", b"data1").unwrap();
        wal.append_for_topic("topic2", b"data2").unwrap();
        
        assert_eq!(wal.read_next("topic1").unwrap().data, b"data1");
        assert_eq!(wal.read_next("topic2").unwrap().data, b"data2");
        
        // Should be empty after reading
        assert!(wal.read_next("topic1").is_none());
        assert!(wal.read_next("topic2").is_none());
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_multiple_entries_same_topic() {
        cleanup_wal();
        let wal = Walrus::new();
        
        let entries = vec![b"entry1", b"entry2", b"entry3", b"entry4"];
        for entry in &entries {
            wal.append_for_topic("multi_topic", *entry).unwrap();
        }
        
        for expected in &entries {
            assert_eq!(wal.read_next("multi_topic").unwrap().data, expected.as_slice());
        }
        
        assert!(wal.read_next("multi_topic").is_none());
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_interleaved_topics() {
        cleanup_wal();
        let wal = Walrus::new();
        
        wal.append_for_topic("a", b"a1").unwrap();
        wal.append_for_topic("b", b"b1").unwrap();
        wal.append_for_topic("a", b"a2").unwrap();
        wal.append_for_topic("b", b"b2").unwrap();
        
        assert_eq!(wal.read_next("a").unwrap().data, b"a1");
        assert_eq!(wal.read_next("b").unwrap().data, b"b1");
        assert_eq!(wal.read_next("a").unwrap().data, b"a2");
        assert_eq!(wal.read_next("b").unwrap().data, b"b2");
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_large_entries() {
        cleanup_wal();
        let wal = Walrus::new();
        
        // Test with various sizes
        let sizes = vec![1024, 64 * 1024, 512 * 1024, 1024 * 1024]; // 1KB to 1MB
        
        for (i, size) in sizes.iter().enumerate() {
            let data = vec![i as u8 + 1; *size];
            wal.append_for_topic("large_test", &data).unwrap();
        }
        
        for (i, size) in sizes.iter().enumerate() {
            let expected = vec![i as u8 + 1; *size];
            let actual = wal.read_next("large_test").unwrap().data;
            assert_eq!(actual, expected);
        }
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_zero_length_entry() {
        cleanup_wal();
        let wal = Walrus::new();
        
        wal.append_for_topic("empty", b"").unwrap();
        wal.append_for_topic("empty", b"not_empty").unwrap();
        
        assert_eq!(wal.read_next("empty").unwrap().data, b"");
        assert_eq!(wal.read_next("empty").unwrap().data, b"not_empty");
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_topic_isolation() {
        cleanup_wal();
        let wal = Walrus::new();
        
        // Write to multiple topics
        for i in 0..10 {
            wal.append_for_topic("topic_a", &[i]).unwrap();
            wal.append_for_topic("topic_b", &[i + 100]).unwrap();
        }
        
        // Read from topic_a only
        for i in 0..5 {
            assert_eq!(wal.read_next("topic_a").unwrap().data, &[i]);
        }
        
        // Read from topic_b - should be independent
        for i in 0..10 {
            assert_eq!(wal.read_next("topic_b").unwrap().data, &[i + 100]);
        }
        
        // Continue reading topic_a from where we left off
        for i in 5..10 {
            assert_eq!(wal.read_next("topic_a").unwrap().data, &[i]);
        }
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_recovery_after_restart() {
        cleanup_wal();
        
        // First instance - write data
        {
            let wal = Walrus::new();
            wal.append_for_topic("recovery_test", b"before_restart").unwrap();
            wal.append_for_topic("recovery_test", b"also_before").unwrap();
            
            // Read one entry
            assert_eq!(wal.read_next("recovery_test").unwrap().data, b"before_restart");
        }
        
        // Second instance - should recover state
        {
            let wal = Walrus::new();
            // Should continue from where we left off
            assert_eq!(wal.read_next("recovery_test").unwrap().data, b"also_before");
            assert!(wal.read_next("recovery_test").is_none());
        }
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_write_after_read_exhaustion() {
        cleanup_wal();
        let wal = Walrus::new();
        
        wal.append_for_topic("test", b"first").unwrap();
        assert_eq!(wal.read_next("test").unwrap().data, b"first");
        assert!(wal.read_next("test").is_none());
        
        // Write more data after exhausting reads
        wal.append_for_topic("test", b"second").unwrap();
        assert_eq!(wal.read_next("test").unwrap().data, b"second");
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_concurrent_topics_different_patterns() {
        cleanup_wal();
        let wal = Walrus::new();
        
        // Topic A: few large entries
        let large_data = vec![0xAA; 100 * 1024]; // 100KB
        wal.append_for_topic("topic_large", &large_data).unwrap();
        wal.append_for_topic("topic_large", &large_data).unwrap();
        
        // Topic B: many small entries
        for i in 0..100 {
            wal.append_for_topic("topic_small", &[i as u8]).unwrap();
        }
        
        // Verify both topics work correctly
        assert_eq!(wal.read_next("topic_large").unwrap().data, large_data);
        assert_eq!(wal.read_next("topic_large").unwrap().data, large_data);
        assert!(wal.read_next("topic_large").is_none());
        
        for i in 0..100 {
            assert_eq!(wal.read_next("topic_small").unwrap().data, &[i as u8]);
        }
        assert!(wal.read_next("topic_small").is_none());
        
        cleanup_wal();
    }
}

mod error_handling_tests {
    use super::*;
    
    #[test]
    fn walrus_handles_invalid_data_gracefully() {
        cleanup_wal();
        let wal = Walrus::new();
        
        // Write valid data first
        wal.append_for_topic("test", b"valid_data").unwrap();
        
        // Corrupt the file
        let path = first_data_file();
        let mut bytes = Vec::new();
        {
            let mut f = OpenOptions::new().read(true).open(&path).unwrap();
            f.read_to_end(&mut bytes).unwrap();
        }
        
        // Corrupt metadata length bytes (first 2 bytes of each block)
        {
            let mut f = OpenOptions::new().write(true).open(&path).unwrap();
            f.seek(SeekFrom::Start(0)).unwrap();
            f.write_all(&[0xFF, 0xFF]).unwrap(); // Invalid metadata length
        }
        
        // Should handle corruption gracefully
        let wal2 = Walrus::new();
        let _result = wal2.read_next("test");
        // Should either return None or handle the error gracefully
        // The exact behavior depends on implementation details
        
        cleanup_wal();
    }
}

mod stress_tests {
    use super::*;
    
    #[test]
    fn walrus_many_small_entries() {
        cleanup_wal();
        let wal = Walrus::new();
        
        let num_entries = 1000;
        
        // Write many small entries
        for i in 0..num_entries {
            let data = format!("entry_{:04}", i);
            wal.append_for_topic("stress_small", data.as_bytes()).unwrap();
        }
        
        // Read them back
        for i in 0..num_entries {
            let expected = format!("entry_{:04}", i);
            let actual = wal.read_next("stress_small").unwrap().data;
            assert_eq!(actual, expected.as_bytes());
        }
        
        assert!(wal.read_next("stress_small").is_none());
        
        cleanup_wal();
    }
    
    #[test]
    fn walrus_multiple_topics_stress() {
        cleanup_wal();
        let wal = Walrus::new();
        
        let num_topics = 10;
        let entries_per_topic = 100;
        
        // Write to multiple topics
        for topic_id in 0..num_topics {
            for entry_id in 0..entries_per_topic {
                let data = format!("t{}_e{}", topic_id, entry_id);
                let topic_name = format!("stress_topic_{}", topic_id);
                wal.append_for_topic(&topic_name, data.as_bytes()).unwrap();
            }
        }
        
        // Read from all topics
        for topic_id in 0..num_topics {
            let topic_name = format!("stress_topic_{}", topic_id);
            for entry_id in 0..entries_per_topic {
                let expected = format!("t{}_e{}", topic_id, entry_id);
                let actual = wal.read_next(&topic_name).unwrap().data;
                assert_eq!(actual, expected.as_bytes());
            }
            assert!(wal.read_next(&topic_name).is_none());
        }
        
        cleanup_wal();
    }
}


