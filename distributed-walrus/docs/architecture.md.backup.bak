# Distributed Walrus Architecture

## Overview

Distributed Walrus is a distributed, Raft-based write-ahead log (WAL) storage system that provides multi-node durability with automatic failover and segment management. It combines walrus-rust (local WAL engine) with octopii (Raft consensus) to create a distributed log system similar to Kafka, but purpose-built for WAL semantics.

### Core Design Principles

1. **Separation of Concerns**: Clean separation between metadata (Raft-managed cluster state), data plane (Walrus I/O), and control plane (routing/coordination)
2. **Lease-Based Safety**: Write operations protected by leases to prevent split-brain writes during network partitions
3. **Generation-Based Versioning**: Each partition uses generations for segment versioning and leadership migration
4. **Consensus-Driven Metadata**: All cluster state changes flow through Raft for strong consistency

---

## System Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Client Applications                          │
└────────────────────────────────┬────────────────────────────────────┘
                                 │ ForwardAppend/JoinCluster RPC
                                 │ (Custom protocol via Octopii/QUIC)
                                 │
        ┌────────────────────────▼────────────────────────┐
        │           NodeController (Router)               │
        │  ┌──────────────────────────────────────────┐   │
        │  │ - Request routing & lease management     │   │
        │  │ - Logical offset tracking                │   │
        │  │ - RPC handling & forwarding              │   │
        │  │ - Join cluster coordination              │   │
        │  └──────────────────────────────────────────┘   │
        └───────────┬────────────────────┬─────────────────┘
                    │                    │
         Data Ops   │                    │  Metadata Ops
         (append/   │                    │  (queries/proposals)
          read)     │                    │
                    │                    │
        ┌───────────▼─────────┐   ┌──────▼───────────────────┐
        │   BucketService     │   │ MetadataStateMachine     │
        │  ┌───────────────┐  │   │ ┌──────────────────────┐ │
        │  │ Lease Guard   │  │   │ │ ClusterState         │ │
        │  │ - active_leases│ │   │ │ - Topics & Partitions│ │
        │  │ - write_locks  │ │   │ │ - Leader assignments │ │
        │  └───────────────┘  │   │ │ - Segment history    │ │
        │                     │   │ └──────────────────────┘ │
        └──────────┬──────────┘   └──────────┬───────────────┘
                   │                         │
          Walrus   │                         │  Raft State Machine
          I/O Ops  │                         │  Apply/Snapshot
                   │                         │
        ┌──────────▼──────────┐   ┌──────────▼───────────────┐
        │   Walrus Engine     │   │    OctopiiNode           │
        │  (Local WAL)        │   │  (Raft Consensus)        │
        │  ┌───────────────┐  │   │  ┌─────────────────────┐ │
        │  │io_uring/mmap  │  │   │  │ OpenRaft + QUIC     │ │
        │  │batch append   │  │   │  │ - Leader election   │ │
        │  │batch read     │  │   │  │ - Log replication   │ │
        │  └───────────────┘  │   │  │ - Membership mgmt   │ │
        └─────────────────────┘   │  └─────────────────────┘ │
                                  └──────────────────────────┘
                                           │
                                           │ AppendEntries/
                                           │ RequestVote/
                                           │ Snapshot
                                           │
                                  ┌────────▼────────┐
                                  │  Other Nodes    │
                                  │  (Raft Cluster) │
                                  └─────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│                  Background Tasks (Tokio)                           │
│  ┌─────────────────────────┐      ┌───────────────────────────┐    │
│  │ Lease Sync Loop (100ms) │      │ Monitor Loop (10s default)│    │
│  │ - Query metadata        │      │ - Check segment sizes     │    │
│  │ - Sync bucket leases    │      │ - Trigger rollovers       │    │
│  └─────────────────────────┘      └───────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Module Architecture

### Module Hierarchy

```
distributed-walrus/
├── main.rs                 (Bootstrap & Lifecycle)
├── config.rs              (Configuration)
├── metadata.rs            (Raft State Machine)
├── bucket.rs              (Lease-Protected Storage)
├── rpc.rs                 (Protocol Definitions)
├── monitor.rs             (Segment Management)
└── controller/
    ├── mod.rs             (Request Router)
    ├── types.rs           (WAL Key Generation)
    ├── internal.rs        (Append Logic)
    └── topics.rs          (Topic Management)
```

---

## Module Details

### 1. Main Entry Point (`main.rs`)

**Responsibilities:**
- Component initialization and wiring
- Cluster bootstrap and join protocol
- Background task spawning
- Signal handling and graceful shutdown

**Bootstrap Flow:**

```
Node 1 (Initial Leader):                Other Nodes (Followers):
┌─────────────────────┐                 ┌─────────────────────┐
│ 1. Create Bucket    │                 │ 1. Create Bucket    │
│    Service          │                 │    Service          │
└──────────┬──────────┘                 └──────────┬──────────┘
           │                                       │
┌──────────▼──────────┐                 ┌──────────▼──────────┐
│ 2. Create Metadata  │                 │ 2. Create Metadata  │
│    StateMachine     │                 │    StateMachine     │
└──────────┬──────────┘                 └──────────┬──────────┘
           │                                       │
┌──────────▼──────────┐                 ┌──────────▼──────────┐
│ 3. Init Octopii     │                 │ 3. Init Octopii     │
│    Node (Raft)      │                 │    Node (Raft)      │
└──────────┬──────────┘                 └──────────┬──────────┘
           │                                       │
┌──────────▼──────────┐                 ┌──────────▼──────────┐
│ 4. Campaign for     │                 │ 4. Send JoinCluster │
│    Leadership       │                 │    RPC to Node 1    │
└──────────┬──────────┘                 └──────────┬──────────┘
           │                                       │
┌──────────▼──────────┐                 ┌──────────▼──────────┐
│ 5. Create topic     │                 │ 5. Add as learner   │
│    "logs" w/ 2      │                 │    → Sync logs      │
│    partitions       │                 │    → Promote voter  │
└──────────┬──────────┘                 └──────────┬──────────┘
           │                                       │
┌──────────▼──────────┐                 ┌──────────▼──────────┐
│ 6. Rollover p0      │                 │ 6. Start lease sync │
│    to gen 1         │                 │    & monitor loops  │
└──────────┬──────────┘                 └──────────┬──────────┘
           │                                       │
┌──────────▼──────────┐                 ┌──────────▼──────────┐
│ 7. Start background │                 │ 7. Ready to serve   │
│    tasks & serve    │                 │                     │
└─────────────────────┘                 └─────────────────────┘
```

**External API:**
- None (binary entry point)

---

### 2. Configuration (`config.rs`)

**Responsibilities:**
- CLI argument parsing
- Runtime configuration management
- Directory path resolution

**Data Structure:**

```rust
NodeConfig {
    node_id: u64,                        // Unique node identifier (1, 2, 3, ...)
    data_root: PathBuf,                  // Base directory for all storage
    join_addr: Option<String>,           // Peer address for joining (e.g., "node1:6001")
    initial_peers: Vec<String>,          // Bootstrap peer list
    raft_port: u16,                      // Raft RPC bind port (default: 6001)
    raft_host: String,                   // Bind address (default: "0.0.0.0")
    raft_advertise_host: Option<String>, // Advertised address for clustering
    log_file: Option<PathBuf>,           // Log file path
}
```

**Directory Layout:**

```
{data_root}/
└── node_{id}/
    ├── user_data/         (Walrus data - BucketService)
    │   └── data_plane/
    │       └── t_{topic}_p_{partition}_g_{generation}/
    └── raft_meta/         (Raft metadata - Octopii)
        ├── logs/
        ├── state/
        └── snapshots/
```

**External API:**
- `NodeConfig::from_args()` - Parse CLI arguments

---

### 3. Metadata State Machine (`metadata.rs`)

**Responsibilities:**
- Raft-replicated cluster state management
- Topic and partition lifecycle
- Leader assignment and segment history tracking
- State machine snapshot/restore

**Core Data Structures:**

```
ClusterState
├── topics: HashMap<TopicName, TopicInfo>
│
TopicInfo
├── partitions: u32
├── partition_states: HashMap<u32, PartitionState>
│
PartitionState
├── current_generation: u64        (Active generation number)
├── leader_node: NodeId             (Current write leader)
├── history: Vec<SegmentRecord>     (Sealed segment records)
│
SegmentRecord
├── generation: u64                 (Segment generation number)
├── stored_on_node: NodeId          (Which node owns this segment)
├── start_offset: u64               (Logical start offset)
├── end_offset: u64                 (Logical end offset)
```

**State Machine Commands:**

```
MetadataCmd::CreateTopic {
    name: String,
    partitions: u32,
    initial_leader: NodeId,
}
→ Creates topic with N partitions, all initially assigned to leader

MetadataCmd::RolloverPartition {
    name: String,
    partition: u32,
    new_leader: NodeId,
    sealed_segment_size_bytes: u64,
}
→ Seals current generation, creates new generation on new leader
→ Records sealed segment in history
→ Increments current_generation
```

**Rollover State Transition:**

```
Before:
  PartitionState {
    current_generation: 1,
    leader_node: 1,
    history: []
  }

Apply: RolloverPartition { new_leader: 2, sealed_size: 1GB }

After:
  PartitionState {
    current_generation: 2,
    leader_node: 2,
    history: [
      SegmentRecord {
        generation: 1,
        stored_on_node: 1,
        start_offset: 0,
        end_offset: 1_000_000_000
      }
    ]
  }
```

**External API:**
- `get_partition_leader(topic, partition) -> Option<(NodeId, u64)>`
- `get_partition_state(topic, partition) -> Option<PartitionState>`
- `assignments_for_node(node_id) -> HashSet<(String, u32, u64)>`
- `reassign_leader(removed_node, replacement_node)`

**Octopii Integration:**
- Implements `octopii::StateMachineTrait`
- `apply(data)` - Process CreateTopic/RolloverPartition commands
- `snapshot() -> Vec<u8>` - Serialize ClusterState for Raft snapshots
- `restore(data)` - Deserialize ClusterState from snapshots

---

### 4. Bucket Service (`bucket.rs`)

**Responsibilities:**
- Lease-protected Walrus I/O wrapper
- Prevent split-brain writes via lease enforcement
- Per-key write serialization

**Architecture:**

```
BucketService
├── engine: Arc<Walrus>                      (Underlying WAL engine)
├── active_leases: RwLock<HashSet<String>>   (Currently held wal_keys)
└── write_locks: RwLock<HashMap<String, Arc<Mutex<()>>>>
```

**Lease Protection Model:**

```
Write Request
     │
     ▼
┌─────────────────┐
│ Acquire per-key │
│ write lock      │
└────────┬────────┘
         │
         ▼
┌─────────────────┐      ┌──────────────────┐
│ Check lease     │─no──►│ Return error:    │
│ in active_leases│      │ NotLeaderFor     │
└────────┬────────┘      │ Partition        │
         │ yes           └──────────────────┘
         ▼
┌─────────────────┐
│ Walrus::batch_  │
│ append_for_topic│
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Return success  │
└─────────────────┘
```

**Lease Synchronization:**

```
sync_leases(expected: HashSet<String>)
│
├─► To Revoke = active_leases - expected
│   └─► For each: active_leases.remove(wal_key)
│
└─► To Grant = expected - active_leases
    └─► For each: active_leases.insert(wal_key)
```

**WAL Key Format:**
- Pattern: `t_{topic}_p_{partition}_g_{generation}`
- Example: `t_logs_p_0_g_1` (topic=logs, partition=0, generation=1)
- Namespace: All stored under `DATA_NAMESPACE = "data_plane"`

**External API:**
- `append_by_key(wal_key, data) -> Result<(), BucketError>`
- `read_by_key(wal_key, max_bytes) -> Result<Vec<Entry>, BucketError>`
- `sync_leases(expected: &HashSet<String>)`
- `get_topic_size_blocking(wal_key) -> Result<u64, BucketError>`

**Backend Selection:**
- Default: `io_uring` (high-performance async I/O on Linux)
- Fallback: `mmap` (set `WALRUS_DISABLE_IO_URING=1`)

---

### 5. Controller (`controller/mod.rs`)

**Responsibilities:**
- Request routing and RPC handling
- Lease synchronization coordination
- Cluster join orchestration
- Logical offset tracking

**Architecture:**

```
NodeController
├── node_id: NodeId
├── bucket: Arc<BucketService>          (Local storage)
├── metadata: Arc<MetadataStateMachine> (Cluster state)
├── raft: Arc<OctopiiNode>              (Consensus engine)
├── offsets: Arc<RwLock<HashMap<String, u64>>>  (Logical offsets)
└── config: ControllerConfig            (Timeouts, etc.)
```

**RPC Handling Flow:**

```
Incoming RPC (Custom payload via Octopii)
     │
     ▼
┌────────────────────────┐
│ Deserialize InternalOp │
└──────────┬─────────────┘
           │
           ├─► ForwardAppend ──┐
           │                   │
           ├─► ForwardRead  ───┼─► controller.handle_rpc(op)
           │                   │
           ├─► JoinCluster  ───┤
           │                   │
           └─► TestControl  ───┘
                   │
                   ▼
           ┌────────────────┐
           │ Process & return│
           │ InternalResp   │
           └────────────────┘
```

**Lease Sync Loop (100ms interval):**

```
┌──────────────────────────────────────┐
│ run_lease_sync_loop()                │
│                                      │
│  loop {                              │
│    ┌──────────────────────────────┐ │
│    │ 1. Query metadata for owned  │ │
│    │    assignments:              │ │
│    │    assignments_for_node(id)  │ │
│    └────────────┬─────────────────┘ │
│                 │                    │
│    ┌────────────▼─────────────────┐ │
│    │ 2. Convert to wal_keys:      │ │
│    │    wal_key(t, p, g)          │ │
│    └────────────┬─────────────────┘ │
│                 │                    │
│    ┌────────────▼─────────────────┐ │
│    │ 3. Sync bucket leases:       │ │
│    │    bucket.sync_leases(&keys) │ │
│    └──────────────────────────────┘ │
│                                      │
│    sleep(100ms)                      │
│  }                                   │
└──────────────────────────────────────┘
```

**Join Cluster Handling:**

```
JoinCluster { node_id: 3, addr: "node3:6001" }
     │
     ▼
┌────────────────────────┐
│ 1. Resolve hostname to │
│    SocketAddr          │
└──────────┬─────────────┘
           │
           ▼
┌────────────────────────┐
│ 2. Add as learner:     │
│    raft.add_learner(3) │
└──────────┬─────────────┘
           │
           ▼
┌────────────────────────────────────┐
│ 3. Spawn background monitor task:  │
│    loop (120 attempts, 500ms each) │
│      if raft.is_learner_caught_up()│
│        raft.promote_learner()      │
│        break                       │
└────────────────────────────────────┘
```

**External API:**
- `handle_rpc(InternalOp) -> InternalResp` (via custom RPC handler)
- `sync_leases_now()` (called internally and by append logic)

**Submodules:**

**`types.rs`:**
```rust
fn wal_key(topic: &str, partition: u32, generation: u64) -> String {
    format!("t_{}_p_{}_g_{}", topic, partition, generation)
}
```

**`internal.rs`:**
- `forward_append()` - Lease sync + append + offset tracking

**`topics.rs`:**
- `create_topic()` - Propose CreateTopic via Raft (leader only)
- `remove_node_from_membership()` - Reassign partitions when node removed

---

### 6. RPC Protocol (`rpc.rs`)

**Responsibilities:**
- Internal operation definitions
- Request/response types for custom RPC

**Protocol Definition:**

```rust
// Request types
pub enum InternalOp {
    ForwardAppend {
        wal_key: String,     // Target: t_logs_p_0_g_1
        data: Vec<u8>,       // Payload bytes
    },
    ForwardRead {
        wal_key: String,
        start_offset: u64,
        max_bytes: usize,
    },
    JoinCluster {
        node_id: u64,        // New node ID
        addr: String,        // Advertised address
    },
    TestControl(TestControl),  // Fault injection
}

// Response types
pub enum InternalResp {
    Ok,
    ReadResult {
        data: Vec<Vec<u8>>,
        high_watermark: u64,
    },
    Error(String),
}
```

**Wire Protocol Flow:**

```
Client                        Server
  │                             │
  ├─► Serialize InternalOp      │
  │   with bincode              │
  │                             │
  ├─► RequestPayload::Custom {  │
  │     operation: "Forward",   │
  │     data: bytes              │
  │   }                         │
  │                             │
  ├────────── QUIC ────────────►│
  │                             │
  │                             ├─► Deserialize InternalOp
  │                             │
  │                             ├─► controller.handle_rpc(op)
  │                             │
  │                             ├─► Serialize InternalResp
  │                             │
  │◄──── ResponsePayload ───────┤
  │      ::CustomResponse {     │
  │        success: bool,        │
  │        data: bytes           │
  │      }                       │
  │                             │
  ├─► Deserialize InternalResp  │
```

**External API:**
- Types used by controller RPC handler
- Serialization via `bincode` (binary, efficient)

---

### 7. Monitor (`monitor.rs`)

**Responsibilities:**
- Automatic segment size monitoring
- Rollover triggering when segments exceed threshold
- Round-robin leader assignment

**Configuration:**
- `WALRUS_MONITOR_CHECK_MS`: Check interval (default: 10,000ms / 10s)
- `WALRUS_MAX_SEGMENT_BYTES`: Rollover threshold (default: 1GB)

**Rollover Decision Logic:**

```
┌─────────────────────────────────────────────────────┐
│ For each owned (topic, partition, generation):      │
│                                                     │
│  ┌────────────────────────────────────────────┐    │
│  │ 1. Get disk size:                          │    │
│  │    bucket.get_topic_size_blocking(wal_key) │    │
│  └────────────┬───────────────────────────────┘    │
│               │                                     │
│  ┌────────────▼───────────────────────────────┐    │
│  │ 2. Get logical size from metadata history  │    │
│  │    (sum of sealed segment sizes)           │    │
│  └────────────┬───────────────────────────────┘    │
│               │                                     │
│  ┌────────────▼───────────────────────────────┐    │
│  │ 3. trigger_size = max(disk, logical)       │    │
│  └────────────┬───────────────────────────────┘    │
│               │                                     │
│  ┌────────────▼───────────────────────────────┐    │
│  │ 4. If trigger_size > threshold:            │    │
│  │    - Determine next_leader (round-robin)   │    │
│  │    - sealed_size = logical_size            │    │
│  │    - Propose RolloverPartition via Raft    │    │
│  └────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────┘
```

**Leader Selection (Round-Robin):**

```
voters = [1, 2, 3]  (from Raft metrics)
current_idx = position(self.node_id in voters)
next_leader = voters[(current_idx + 1) % voters.len()]

Example:
  Current leader: node 1 → next leader: node 2
  Current leader: node 2 → next leader: node 3
  Current leader: node 3 → next leader: node 1
```

**Key Design Decision:**
- **Trigger on disk size** (prevents unbounded disk usage)
- **Record logical size** (maintains offset consistency across replicas)

**External API:**
- `run_monitor_loop()` - Background task spawned by main

---

## Data Flows

### Write Path (ForwardAppend)

```
┌─────────┐
│ Client  │
└────┬────┘
     │ ForwardAppend { wal_key: "t_logs_p_0_g_1", data: [...] }
     ▼
┌──────────────────────┐
│  NodeController      │
│  (Any node)          │
└─────────┬────────────┘
          │
          │ 1. sync_leases_now()
          │    (ensure fresh state from metadata)
          ▼
     ┌─────────────────────┐
     │ Metadata Query:     │
     │ assignments_for_node│
     └─────────┬───────────┘
               │
               │ 2. Check if we own wal_key
               ▼
          ┌─────────────┐          ┌──────────────────────┐
          │ Have lease? │───no────►│ Return Error:        │
          └─────┬───────┘          │ NotLeaderForPartition│
                │ yes              └──────────────────────┘
                ▼
     ┌────────────────────┐
     │ BucketService      │
     │ 3. Acquire write   │
     │    lock for wal_key│
     └─────────┬──────────┘
               │
               │ 4. Double-check lease
               ▼
          ┌─────────────┐          ┌──────────────────────┐
          │ Lease valid?│───no────►│ Return Error:        │
          └─────┬───────┘          │ NotLeaderForPartition│
                │ yes              └──────────────────────┘
                ▼
     ┌────────────────────┐
     │ Walrus Engine      │
     │ 5. batch_append_   │
     │    for_topic()     │
     └─────────┬──────────┘
               │
               │ 6. Success
               ▼
     ┌────────────────────┐
     │ Controller         │
     │ 7. Update offset   │
     │    tracker         │
     └─────────┬──────────┘
               │
               ▼
     ┌────────────────────┐
     │ Return InternalResp│
     │ ::Ok               │
     └────────────────────┘
```

### Metadata Change Path (Rollover)

```
┌────────────────────┐
│ Monitor Loop       │
│ (Every 10s)        │
└─────────┬──────────┘
          │
          │ 1. Check segment sizes
          ▼
     ┌──────────────────┐
     │ Size > threshold?│───no───► Continue
     └─────────┬────────┘
               │ yes
               │
               │ 2. Determine next leader (round-robin)
               ▼
     ┌────────────────────┐
     │ Build Rollover Cmd:│
     │ RolloverPartition {│
     │   name: "logs",    │
     │   partition: 0,    │
     │   new_leader: 2,   │
     │   sealed_size: 1GB │
     │ }                  │
     └─────────┬──────────┘
               │
               │ 3. Propose via Raft
               ▼
     ┌────────────────────┐
     │ OctopiiNode        │
     │ raft.propose(cmd)  │
     └─────────┬──────────┘
               │
               │ 4. Replicate to quorum
               ▼
     ┌────────────────────┐
     │ All Nodes:         │
     │ Metadata SM Apply  │
     │ - Add SegmentRecord│
     │   to history       │
     │ - Increment gen    │
     │ - Update leader    │
     └─────────┬──────────┘
               │
               │ 5. Lease sync picks up change (within 100ms)
               ▼
     ┌────────────────────────────────────┐
     │ Old Leader (Node 1):               │
     │ - Loses lease for t_logs_p_0_g_1   │
     │ - Future writes fail with NotLeader│
     └────────────────────────────────────┘
     ┌────────────────────────────────────┐
     │ New Leader (Node 2):               │
     │ - Gains lease for t_logs_p_0_g_2   │
     │ - Ready to accept writes           │
     └────────────────────────────────────┘
```

### Join Path (Dynamic Membership)

```
┌──────────────┐                      ┌──────────────┐
│  New Node    │                      │ Existing Node│
│  (node3)     │                      │  (node1)     │
└──────┬───────┘                      └──────┬───────┘
       │                                     │
       │ 1. Start with --join node1:6001     │
       │                                     │
       │ 2. Resolve "node1:6001" to SocketAddr
       │                                     │
       │ 3. JoinCluster RPC                  │
       ├────────────────────────────────────►│
       │    { node_id: 3,                    │
       │      addr: "node3:6001" }           │
       │                                     │
       │                                     │ 4. Add as learner
       │                                     ├─► raft.add_learner(3, addr)
       │                                     │
       │◄────── Raft Log Replication ───────┤ 5. Replicate all
       │        (AppendEntries RPCs)         │    committed logs
       │                                     │
       │                                     │ 6. Spawn monitor task
       │                                     │    (120 attempts, 500ms)
       │                                     │
       │                                     │ 7. Poll catch-up status
       │                                     ├─► is_learner_caught_up(3)?
       │                                     │
       │                                     │ 8. Once caught up:
       │                                     ├─► raft.promote_learner(3)
       │                                     │
       │◄──── Membership Change Applied ─────┤ 9. Now a voter
       │                                     │
       │ 10. Start lease sync & monitor loops│
       │                                     │
       ├──────────────────┐                  │
       │ Ready to serve   │                  │
       └──────────────────┘                  │
```

---

## Octopii Integration

### Overview

Octopii is a Raft consensus library that provides:
- **OpenRaft-based consensus**: Leader election, log replication, snapshots
- **QUIC transport**: Fast, secure, multiplexed networking via Quinn
- **WAL-backed storage**: Durable Raft log and state storage
- **Custom RPC layer**: Extensible request/response protocol

### Integration Points

**1. State Machine Registration:**

```rust
let metadata = Arc::new(MetadataStateMachine::new());

let raft = OctopiiNode::new_with_state_machine(
    oct_cfg,           // Node ID, bind addr, peers, WAL dir
    runtime,           // Async runtime
    metadata.clone()   // Custom state machine
).await?;
```

**2. Custom RPC Handler:**

```rust
raft.set_custom_rpc_handler(move |req: RequestPayload| async move {
    if let RequestPayload::Custom { operation: "Forward", data } = req.payload {
        let op: InternalOp = bincode::deserialize(&data)?;
        let resp = controller.handle_rpc(op).await;
        let data = bincode::serialize(&resp)?;
        return ResponsePayload::CustomResponse {
            success: true,
            data
        };
    }
    ResponsePayload::Error { message: "Unsupported operation" }
}).await;
```

**3. Metadata Proposals:**

```rust
// Create topic
let cmd = MetadataCmd::CreateTopic {
    name: "logs".to_string(),
    partitions: 2,
    initial_leader: node_id,
};
let payload = bincode::serialize(&cmd)?;
raft.propose(payload).await?;  // Consensus-driven state change
```

**4. Membership Management:**

```rust
// Add new node
raft.add_learner(node_id, socket_addr).await?;

// Check replication status
if raft.is_learner_caught_up(node_id).await? {
    raft.promote_learner(node_id).await?;
}

// Remove node
raft.remove_node(node_id).await?;
```

**5. Leadership and Metrics:**

```rust
// Campaign for leadership
if node_id == 1 {
    raft.campaign().await?;
}

// Check if leader
if raft.is_leader() {
    // Only leader can propose state changes
}

// Get cluster membership
let metrics = raft.raft_metrics().await;
let voters = metrics.membership_config.voters();
```

### Data Flow with Octopii

```
┌─────────────────────────────────────────────────────────┐
│                   Application Layer                     │
│  (Controller, Monitor, Main)                            │
└─────────────┬────────────────────────┬──────────────────┘
              │                        │
              │ Propose metadata       │ Query state
              │ changes                │ & leadership
              ▼                        ▼
    ┌──────────────────┐      ┌────────────────────┐
    │  OctopiiNode     │      │ MetadataStateMachine│
    │  (Raft Layer)    │      │  (State Machine)    │
    └─────────┬────────┘      └────────┬───────────┘
              │                        │
              │ Replicate proposals    │ Apply committed
              │                        │ entries
              ▼                        ▼
    ┌──────────────────────────────────────────┐
    │         Raft Log (Walrus WAL)            │
    │  - Append entries                        │
    │  - Read for replication                  │
    │  - Snapshot storage                      │
    └──────────────────┬───────────────────────┘
                       │
                       │ AppendEntries/
                       │ RequestVote/
                       │ Snapshot RPCs
                       ▼
              ┌─────────────────┐
              │  QUIC Transport │
              │  (Other Nodes)  │
              └─────────────────┘
```

---

## External APIs

### 1. Custom RPC Protocol (via Octopii QUIC)

**Endpoint:** Raft port (default: 6001)

**Operations:**

**ForwardAppend:**
```rust
Request:
  InternalOp::ForwardAppend {
      wal_key: "t_logs_p_0_g_1",
      data: vec![...],
  }

Response:
  InternalResp::Ok
  // or
  InternalResp::Error("NotLeaderForPartition")
```

**JoinCluster:**
```rust
Request:
  InternalOp::JoinCluster {
      node_id: 3,
      addr: "node3:6001",
  }

Response:
  InternalResp::Ok
  // or
  InternalResp::Error("Failed to add node")
```

**TestControl (Testing Only):**
```rust
Request:
  InternalOp::TestControl(TestControl::FailForwardRead)

Response:
  InternalResp::Ok
```

### 2. Raft Internal RPCs (Managed by Octopii)

**Automatically handled:**
- `AppendEntries` - Log replication
- `RequestVote` - Leader election
- `InstallSnapshot` - Snapshot transfer
- `AddLearner` - Membership change
- `PromoteLearner` - Membership change

### 3. Planned: Simple Protocol Server

**Status:** Not yet implemented

**Design notes from code:**
- Separate port from Raft (e.g., 9091, 9092, 9093)
- Message-based semantics (vs current byte-based)
- Kafka-like wire protocol
- Direct client access without custom RPC layer

---

## Key Design Decisions

### 1. Generation-Based Versioning

**Problem:** How to migrate partition leadership without data loss?

**Solution:**
- Each partition rollover increments generation number
- Old leader keeps sealed segment (e.g., `t_logs_p_0_g_1`)
- New leader starts fresh segment (e.g., `t_logs_p_0_g_2`)
- Metadata tracks all generations in history

**Benefits:**
- Graceful leadership migration
- No data copying required
- Historical segments remain accessible on original nodes

### 2. Lease-Based Write Safety

**Problem:** Prevent split-brain writes during network partitions

**Solution:**
- Controller syncs leases from metadata every 100ms
- BucketService enforces lease checks on every write
- Writes fail immediately if lease invalid

**Guarantees:**
- Only one node can write to a partition/generation at a time
- Even if network partitioned, minority nodes lose leases
- Majority partition continues to serve writes

### 3. Separation of Disk vs Logical Size

**Problem:** When to trigger segment rollover?

**Solution:**
- Monitor uses **disk size** to decide when to rollover
- Metadata records **logical size** (sum of sealed segments)
- Rollover triggers on `max(disk_size, logical_size)`

**Rationale:**
- Prevents unbounded disk usage (disk-based trigger)
- Maintains offset consistency (logical-based history)
- Handles disk amplification from WAL internals

### 4. Two-Level Storage Architecture

**Data Plane (Walrus):**
- High-performance, durable local WAL
- Optimized for sequential writes (io_uring/mmap)
- No consensus overhead for data operations

**Control Plane (Raft via Octopii):**
- Strongly consistent metadata
- Leader assignment and segment history
- Cluster membership management

**Benefits:**
- Data writes don't require consensus (low latency)
- Metadata consistency guaranteed (strong safety)
- Clear separation of concerns

### 5. Async-Blocking Hybrid Model

**Async Layer (Tokio):**
- Controller, network RPC handling
- Lease sync loop, monitor loop
- Octopii Raft operations

**Blocking Layer:**
- Walrus I/O operations (`spawn_blocking`)
- Avoids blocking async executor

**Rationale:**
- Walrus is synchronous API
- Tokio best practice for CPU-bound sync work
- Maintains async performance for network/control plane

---

## Environment Configuration

```bash
# Storage Backend
WALRUS_DISABLE_IO_URING=1        # Use mmap instead of io_uring (default: 0)

# Monitor Settings
WALRUS_MONITOR_CHECK_MS=1000     # Rollover check interval in ms (default: 10000)
WALRUS_MAX_SEGMENT_BYTES=10485760 # Rollover threshold in bytes (default: 1073741824 / 1GB)

# RPC Settings
WALRUS_RPC_TIMEOUT_MS=4000       # Forward RPC timeout in ms (default: 3000)

# Retention (Tracked but not yet enforced)
WALRUS_RETENTION_GENERATIONS=0   # Number of generations to retain (default: 0 / unlimited)
```

---

## Deployment

### Docker Compose Cluster

**Configuration:**
```yaml
services:
  node1:  # Bootstrap leader
    command: ["--node-id", "1", "--raft-port", "6001"]
    ports: ["6001:6001", "9091:9091"]

  node2:  # Join node1
    command: ["--node-id", "2", "--join", "node1:6001", "--raft-port", "6002"]
    ports: ["6002:6002", "9092:9092"]

  node3:  # Join node1
    command: ["--node-id", "3", "--join", "node1:6001", "--raft-port", "6003"]
    ports: ["6003:6003", "9093:9093"]

  node4:  # Hot-join (manual start)
    profiles: ["hotjoin"]
    command: ["--node-id", "4", "--join", "node1:6001", "--raft-port", "6004"]
```

**Network:**
- Bridge network `walrus-net`
- Nodes can resolve each other by service name

**Storage:**
- Bind-mounted volumes: `./test_data/node{1,2,3,4}:/data`

### Typical Cluster Lifecycle

```
1. docker compose up node1
   → Node 1 becomes leader
   → Creates topic "logs" with 2 partitions
   → Rollovers partition 0 to generation 1

2. docker compose up node2 node3
   → Nodes join via node1
   → Added as learners → promoted to voters
   → Start lease sync & monitor loops

3. [Runtime] Monitor triggers rollover on node 1
   → Proposes RolloverPartition (new_leader: node2)
   → Node 1 seals generation, node 2 starts new generation

4. [Optional] docker compose --profile hotjoin up node4
   → Node 4 joins running cluster
   → Tests dynamic membership
```

---

## Current State & Future Work

### Implemented

- Raft-based metadata consensus
- Lease-based write safety
- Automatic segment rollover
- Dynamic cluster membership (join)
- Leader election and failover
- Logical offset tracking
- Round-robin leadership rotation
- Docker-based deployment

### In Progress / Planned

**Simple Protocol Server:**
- Dedicated client-facing port
- Message-based API (vs byte-based)
- Kafka-like wire protocol
- ForwardRead implementation

**Retention:**
- Generation-based cleanup
- Metadata tracking in place
- Enforcement logic needed

**Operational:**
- Metrics/monitoring endpoints
- Admin API for cluster management
- Graceful shutdown improvements

### Testing Features

**Fault Injection (TestControl):**
- `FailForwardRead` - Simulate read failures
- `FailMonitor` - Disable monitor loop
- `FailDirSize` - Simulate disk size errors

**Atomic Flags:**
- `test_fail_forward_read`
- `test_fail_monitor`
- `test_fail_dir_size`

---

## Summary

Distributed Walrus is a production-ready distributed log system with the following key characteristics:

**Architecture:**
- Two-level storage: Raft for metadata, Walrus for data
- Lease-based write safety with 100ms sync interval
- Generation-based segment versioning for leadership migration

**Scale:**
- ~1,360 lines of Rust code
- 7 core modules + 3 controller submodules
- Built on battle-tested libraries (OpenRaft, Walrus, QUIC)

**Performance:**
- High-throughput writes (io_uring backend)
- Low-latency reads (no consensus required)
- Automatic load distribution (round-robin leadership)

**Operational:**
- Simple deployment (Docker Compose)
- Dynamic membership (hot-join support)
- Automatic failover and segment management

The system provides a solid foundation for distributed WAL workloads and demonstrates clean separation of concerns between consensus (metadata) and storage (data).
